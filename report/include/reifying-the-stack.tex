\chapter{Reifying the Stack} \label{chp:reifying_the_stack}

In section~\ref{sec:recent_work} we saw Peter Wortmann's prototype for
execution stack based stack traces. It was demonstrated in August
2013 \cite{stack_traces_ticket}. This is a rough sketch of how it is
implemented:

\begin{enumerate}
  \item
    The program starts by installing a catch-all handler. This handler will
    print the stack trace on a crash.
  \item
    Program runs and crashes.
  \item
    The run time system handles the crash by walking through the whole
    execution stack and saving it in a separate array. It then invokes the
    installed handler and passes the array.
\end{enumerate}

To store the essential contents of the stack in a new value,
possibly for later trace-printing, is called \emph{reifying} the
stack. The reification from Peter Wortmann's demonstration is
quite simple, figure~\ref{fig:reification} illustrates
this method of reifying the stack.

\begin{figure}
\begin{mdframed}
  \includegraphics[width=5in]{build/fig/reification}
  \caption{An illustration of stack reification, the rightmost box
  is the array allocated to store the result of the reification, the
  diamond arrows show where the content is copied from.}
  \label{fig:reification}
\end{mdframed}
\end{figure}

This chapter will go into
stack reification in detail. By critically looking
at the prototype, we find room both for improvement and discussion.
In section~\ref{sec:frames_of_interest} we see that the stack traces can become more
readable by using the extra information in the payloads of each stack
frame. Section~\ref{sec:lazy_reification} deals with the issue of wasting
resources when reifying the stack without ever using it.

\section{Frames of interest} \label{sec:frames_of_interest}

Figure~\ref{fig:traces} showed that the stack trace from Peter Wortmann's
demonstration is very far from the ideal stack trace.  Worse, the only
information we have to work with is the execution stack, remember, we maintain no
explicit call stack for performance reasons. Still, the stack can become
clearer by using the payload of the stack frames. For the readers convenience,
we reproduce the stack trace here again as figure~\ref{fig:traces_2}.

\begin{figure}
\begin{mdframed}
  \begin{minted}[gobble=4]{text}
     0: stg_bh_upd_frame_ret
     1: stg_bh_upd_frame_ret
     2: stg_bh_upd_frame_ret
     3: showSignedInt
     4: stg_upd_frame_ret
     5: writeBlocks
     6: stg_ap_v_ret
     7: bindIO
     8: bindIO
     9: bindIO
    10: bindIO
    11: stg_catch_frame_ret
  \end{minted}
        \caption{Stack trace. (Same as in figure~\ref{fig:trace_goal1})}\label{fig:traces_2}
\end{mdframed}
\end{figure}

In section~\ref{sec:update_frames} and~\ref{sec:the_other_frames}
we will revisit the stack frames we looked at in
section~\ref{sec:members_of_stack}. This time we will look at how
their frame-specific payload can be utilized to make the stack
traces more useful. In addition to utilizing the existing frames, we
take a stab at creating our own artificial stack frame in
section~\ref{sec:artificial_frames} that could improve stack traces.

\subsection{Update frames} \label{sec:update_frames}

Frame 0,1,2 and 4 from figure~\ref{fig:trace_goal1} are all update
frames. Recall that the code for a thunk will push one update
frame. Thunks are common in Haskell and therefore update frames are common
as well. If we were able to print something better
than the cryptic \texttt{stg\_bh\_upd\_frame} and \texttt{stg\_upd\_frame},
many frames in the stack trace would improve.

The actual code of the thunk that pushed the update frame
could be anything. To only see the info pointer of the update frames is hardly helpful.
But we know that the field of the update frame is a reference to the
heap object the update frame is going to update, the \emph{updatee}. The updatee's
info table contains the code that pushed the update frame.
This is really good, because we can copy that info pointer to our array instead,
as illustrated in figure~\ref{fig:reify_update_frame}.
Unfortunately this trick only works in some cases and it will depend on the kind of
update frame we have on the stack. There are three different kinds of
update frames \cite{github_updates_cmm}.

\begin{figure}
\begin{mdframed}
  \includegraphics[width=5in]{build/fig/reify_update_frame}
  \caption{An improved way of reifying update frames. The old method is
  showed in the dashed line.}
  \label{fig:reify_update_frame}
\end{mdframed}
\end{figure}

\begin{itemize}
  \item
    \texttt{stg\_bh\_upd\_frame} is the update frame used for global
      thunks. Global thunks are better known as CAFs.
  \item
    \texttt{stg\_upd\_frame} is the update frame used for local
  thunks.
  \item
     \texttt{stg\_marked\_upd\_frame} is created
  by overwriting one of the other two kinds of frames, it
  happens during garbage collection and this phase is named
  \emph{blackholing}. Blackholing converts any update frame to a
  marked update frame \cite{github_overwrite_update_frame} and
  overwrites the updatee's info pointer to point to a "black hole"
  \cite{github_overwrite_blackhole}.
\end{itemize}

Unfortunately, only the updatee of local thunks point
at interesting code and not a black hole. The marked update frame
always gets its updatee's info pointer overwritten to a black
hole \cite{github_overwrite_blackhole} and the update frame
for global thunks' updatee point at a black hole to begin with
\cite{github_set_hdr_caf_blackhole}.\footnote{The names
  \texttt{stg\_bh\_upd\_frame} and \texttt{stg\_marked\_upd\_frame} are very
  confusing. In both cases they point at black holes. A
  \texttt{stg\_marked\_upd\_frame} points at a \texttt{BLACKHOLE} and a
  \texttt{stg\_bh\_upd\_frame} points at a \texttt{CAF\_BLACKHOLE}. The names
  are kept true to their name in the GHC source code to ease verifiability.}


Blackholing have become a quite complicated part of the run-time system,
having multiple purposes \cite{ghc_new_implementation_black_holes}
and is implemented with low-level tricks like pointer tagging. But the
traditional black hole as described in \cite{jones1992tail} is having
clear purposes, blackholing fixes a class of space leaks and it detects
some cases of nontermination. In concurrent Haskell, blackholing have
synchronization purposes that increases sharing (avoids re-computation). Blackholing
is illustrated in figure~\ref{fig:thunks_and_blackholes}.

\begin{figure}
\begin{mdframed}
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=2.6in]{build/fig/thunk}
    \caption{A thunk}
  \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[t]{0.5\textwidth}
    \includegraphics[width=2.8in]{build/fig/blackhole}
    \caption{A blackhole}
  \end{subfigure}
  \caption{The same thunk, before and after blackholing.}\label{fig:thunks_and_blackholes}
\end{mdframed}
\end{figure}

\subsubsection{Retaining code reference on blackholing}

The bad effect of blackholing from a stack trace perspective is
that thunks lose the reference to the code that pushed the update frame (recall figure~\ref{fig:reify_update_frame} and figure~\ref{fig:thunks_and_blackholes}). So instead
of being able to print something useful, like  \texttt{... 4: print ...},
we have to print \texttt{... 4: stg\_upd\_frame\_ret ...}.

Unfortunately, blackholing is not optional
\cite{github_blackholing_not_optional}. It is however possible to
retain the reference by just adding a field for all thunks and
copying the reference there. The extended field is shown in figure~\ref{fig:different_layout}.

\begin{figure}
\begin{mdframed}
  \begin{subfigure}[t]{1\textwidth}
    \includegraphics[width=5.5in]{build/fig/old_thunk}
    \caption{The current layout}
  \end{subfigure}
  \vskip2em
  \begin{subfigure}[t]{1\textwidth}
    \includegraphics[width=5.5in]{build/fig/new_thunk}
    \caption{A layout that has a copy of the original info
pointer, since the first info pointer will be overwritten during
blackholing. }
  \end{subfigure}
  \caption{Two different layout for thunks.}\label{fig:different_layout}
\end{mdframed}
\end{figure}

The extra field has a runtime cost of course. The number of thunks
ever created during the lifetime of a Haskell program has no upper bound. However,
there is only a constant number of global thunks, which means that the
performance cost would be insignificant if we only applied the idea from
figure~\ref{fig:different_layout} to global thunks. When
only changing the global thunks, we are able to identify
the \texttt{stg\_bh\_upd\_frame} frames but not the
\texttt{stg\_marked\_upd\_frame} frames. For example, the
segment

\begin{minted}[gobble=4]{text}
     0: stg_bh_upd_frame_ret
     1: stg_bh_upd_frame_ret
     2: stg_bh_upd_frame_ret
     ...
\end{minted}

could instead be

\begin{minted}[gobble=4]{text}
     0: divZeroError
     1: crashSelf
     2: c
     ...
\end{minted}

by utilizing the new payload. The second stack trace is of course
\emph{much} more useful than the first stack trace. Unfortunately,
to only add the field for global thunks will only be useful for a
short time of the life of the thunk, since blackholing is happening
intermittently. If instead all thunks had the extra field, the blackhole
thunks would also retain the useful reference. Luckily though, if a
global thunk is containing code that crashes, it is probable that its
update frame will be left intact if it crashes early. In
fact, we should consider ourselves extra lucky that top-level values
like \texttt{divZeroError} memoize. If there were no memoization,
there would be no thunk or no update frames, instead there would be
regular tail calls and the first three frames would not even be on the stack.

\subsection{The other frames} \label{sec:the_other_frames}

Some of the stack frames don't need any analysis because their info
pointer is what we want to print, other frames seem hopeless and are not
investigated in depth. In this subsection we will give a brief mention
of the frames that we have not analyzed in depth.

For \emph{case continuations}, the info pointer is pointing to code
generated from Haskell, so we consider case continuations as a
trivial frame. When having an stack frame for a \emph{Call continuations}, we've already
jumped to the interesting code and just left a trace of leftover arguments.
There is nothing to do for call continuations since the leftover arguments
say nothing about the over-applied function itself.
\emph{Catch frames} will hold a reference to the handler, but it is
not what is currently evaluating. The good news is that just seeing
any catch frame is helpful, with some context the programmer might be
able to determine which \texttt{catch} in the source code pushed the
catch frame.

\subsection{Artificial frames} \label{sec:artificial_frames}

% TODO: Peter said:
%
% You might want to note that you're jumping from something existing (update
% frames) to one of your proposals at this point. Maybe start with the
% motivation, then say what you're introducing to solve it?

Artificial frames are stack frames whose only purpose is to improve
the stack trace. The intended use case is for programmers that have
got a reproducible bug and a stack trace without enough information.
This should be a common situation, because stack traces derived from the
execution stack often lack frames due to tail calls. A versatile
programmer might switch to one of the many other stack trace
implementations from section~\ref{sec:overhead_full}.
But in practice that is usually an overwhelming task
due to technical difficulties like compiler flags and missing libraries. Instead we present
the programmers with an inbuilt primitive to force a stack frame.
When a stack is later reified, these artificial frames would work as
checkpoints and be printed.

There are two fundamentally different approaches here. Either you create
an actual function like \texttt{pushStackFrame :: a -> a} that takes
one argument, pushes its arguments info pointer on the execution stack
(encapsulated in a special stack frame) and then evaluates the argument.
The other approach is to create a ``macro''-like function that will
expand \texttt{(forceCaseContinuation x)} to \texttt{(case x of \_ ->
x)} in such a way that the case expression can not be optimized away.
This approach will create a unique case continuation for every use site
of \texttt{forceCaseContinuation}. The fundamental difference is that
the first approach will save the \emph{jump target} in the artificial
frame's field. Meanwhile the second approach pushes a case continuation
that will uniquely identify the \emph{jump source}. It is not obvious
which method will be most useful and do what the programmer expects. One
nice property of the jump source implementation is that it will include
function $f$ in the stack trace if \texttt{forceCaseContinuation} is
put inside function $f$ and its argument is currently evaluating.
Further, the jump target approach has a pitfall: When a nontrivial
expression is passed to it, the nontrivial expression will be put into
a thunk that will have a source location associated to it from the jump
source. So the jump target will often be the same as
the jump source. The programmers using \texttt{pushStackFrame} should
therefore understand when GHC decides to create thunks. On the other
hand, \texttt{pushStackFrame} can be the right tool for the job and will
push an artificial stack frame containing an interesting jump target.

\section{Efficient reification} \label{sec:lazy_reification}

When executing the program from figure~\ref{fig:sample_program}, the reification need to happen at the crash site and
not in the handler. When control has been passed to the handler the
execution stack has been unrolled already, so the stack is inaccessible
and maybe even overwritten. At the same time, there is a cost associated
with reifying the stack and the cost is growing linearly with the size
of the execution stack. So when we \emph{know} that we're going to print
the stack, it is acceptable to have the additional linear cost of
reifying the stack, but we shouldn't tolerate the cost when we are not
using the reified stack value. One consequence of always reifying the
stack is that functions that use \texttt{throw} for control flow will
become slower depending on how big the stack is when they're called.
To have the time complexity of a total Haskell function depending on
the state of the underlying runtime system is not acceptable for a mature
compiler like GHC. Note that control flow can be an anticipated use case for
exceptions \cite{peyton1999semantics} (however, a more recent paper
does not anticipate it \cite{marlow2006extensible}). \texttt{lazyReadBuffered} uses exception handling
for control flow \cite{github_lazyReadBuffered_control_flow}.
In this section we will look for alternative solutions
to unconditional and strict stack reification.

Before looking at theoretical solutions, we will test our hypothesis
and convince ourselves by experiment that stack reification really does
take linear time. Figure~\ref{fig:benchmark_program} shows a Haskell
program that runs the exact same pure computation twice, the first time
it runs the computation with a small execution stack and the second time
a lot of frames have been pushed on the execution stack. We also run the
\emph{program} twice, the first time with stack traces enabled and the
second time with stack traces disabled:

\begin{figure}
\begin{mdframed}
  \begin{minted}[gobble=4]{haskell}
    import Control.Exception ( catch
                             , SomeException (SomeException)
                             , evaluate)
    import System.IO.Unsafe (unsafePerformIO)
    import Data.List (foldl')
    import System.CPUTime (getCPUTime)

    main :: IO()
    main = do
        evaluate (stackBuilder 1) -- For fairness with CAFs
        putStrLn $ "Takes " ++ show fast ++ " ms with small stack"
        putStrLn $ "Takes " ++ show slow ++ " ms with large stack"
      where
        fast = stackBuilder 0
        slow = stackBuilder 1000

    getMilliSeconds :: IO Integer
    getMilliSeconds = fmap (`div` 1000000000) getCPUTime

    timeIt :: Integer -> IO Integer
    timeIt x = do t0 <- getMilliSeconds
                  evaluate x
                  t <- getMilliSeconds
                  return (t - t0)

    -- Returns the time it takes to evaluate pureFunction
    stackBuilder :: Integer -> Integer
    stackBuilder x | x < 1 = unsafePerformIO (timeIt (pureFunction x))
    stackBuilder x         = x - x + stackBuilder (x-1) -- Push frames

    -- It has to take an argument to not become a thunk
    pureFunction :: Integer -> Integer
    pureFunction zero =
        foldl' (+) 0 (map stupidFunction [1..(zero + 1000000)])

    stupidFunction :: Integer -> Integer
    stupidFunction x = unsafePerformIO (action `catch` handler)
      where
        action = evaluate (5 `div` 0)
        handler (SomeException _) = return x
  \end{minted}
  \caption{A Haskell program we use for benchmarking purposes.}
  \label{fig:benchmark_program}
\end{mdframed}
\end{figure}

\begin{minted}[gobble=2, samepage=true]{bash}
  $ ./Benchmark +RTS --stack-trace -RTS
  Takes 332 ms with small stack
  Takes 18344 ms with large stack

  $ ./Benchmark
  Takes 175 ms with small stack
  Takes 178 ms with large stack
\end{minted}

The results are exactly what we expect. If we do not reify the stack
at all like in the second program run. The two functions are equally
fast. But if we do one stack reification per exception (as happens in
\texttt{stupidFunction}), the program gets slower as seen from the
results in the first program run. We can also see that the program
gets \emph{much} slower if we the execution stack is already big. This
is considered to be a serious problem because \texttt{pureFunction}
is not supposed to depend on its environment. Note that we do not
bother to mention the exact compiler version, flags, operating system and
hardware, because we will only compare these numbers against each other.

As we just saw, reifying the stack is costly and we will dedicate this whole
section to find more efficient alternatives. The two natural solutions are to
either reify the stack \emph{conditionally} or
to reify it \emph{lazily}. When reifying the stack conditionally, we only reify the stack
when we know for sure that we are going to print the stack. Subsection~\ref{reify_static_analysis} looks at solutions in this category.
The second approach would be to let the stack value be lazily evaluated. If creating the thunk for
the execution stack value could be done in constant time, we have a
satisfactory solution. Subsection~\ref{sec:stack_freezing}
and~\ref{sec:chunked_reifying} are implementation ideas for lazy stack values.
But first we take a look at a simple and
unsophisticated solution in subsection~\ref{sec:constant_frames}.

\subsection{Reifying a constant number of frames} \label{sec:constant_frames}

A very simple solution would be to just reify a constant number of
frames. The time complexity for any reification would then be constant.
The exact number of frames could be specified through a run-time parameter
or from Haskell-land. There is also a few other benefits with this
approach, for one there would be no user-interface issues with too
long stack traces being printed to the screen, second, as we saw from subsection~\ref{sec:update_frames}, the top of the stack is less likely to have
been blackholed. The drawback is that the stack trace could
be too truncated to be useful.

To verify our speculations, we run the benchmarking program from
figure~\ref{fig:benchmark_program} with the frame size limits $10$ and $500$:

\begin{minted}[gobble=2, samepage=true]{bash}
  $ ./Benchmark +RTS --stack-trace --reify-x-frames 10 -RTS
  Takes 232 ms with small stack
  Takes 236 ms with large stack

  $ ./Benchmark +RTS --stack-trace --reify-x-frames 500 -RTS
  Takes 315 ms with small stack
  Takes 4120 ms with large stack
\end{minted}

This makes sense. In the case with a cap on reifying $10$ frames, we see
that both computations take about the same time. It also takes more time
than when not reifying a stack at all (since $10 > 0$), but it takes
less time than when the stack is unbounded, since $10$ is less than the
number of frames on the stack. For the case when reifying $500$ frames,
we see that the computation on a small stack is taking as much time as
it did then we reified the whole stack, this is because the whole stack
is less than $500$ frames. The computation done with the larger stack takes
about one fourth of what it was when we reified the whole stack, this is because
we call \texttt{stackBuilder} $1000$ times, and each time it pushes two frames
(one addition, one subtraction).

\subsection{Static analysis} \label{reify_static_analysis}

By analyzing Haskell source code at compile time we can get
information that could help decide whether we should reify the stack or
not. For instance, when the compiler generate code for the \texttt{throw}
primitive operation, the \emph{compiler} could choose if a stack should be reified or
not at this particular usage of \texttt{throw}. Another idea is to do
static analysis on the uses of the \texttt{catch} primitive, one could mark
the catch frames that are using the stack value and choose at
run-time to reify the stack if the catch frame needs it.

To do static analysis on the uses of \texttt{catch} has no runtime cost, but
there many problems. When the execution stack contains a series of catch
statements and if the topmost catch frame indicates that it doesn't need the stack,
it will not be possible to rethrow the stack trace to the second catch frame which
might need it. Section~\ref{sec:rethrowing_the_stack} in the next chapter
will discuss semantics of rethrowing in detail.

Another serious obstacle with static analysis is that the code
generation phase would require a lot more context to decide what kind
of catch or throw site it is. Catch and throw sites have to generate
code defensively (like with all compiler optimizations). While code from
remote libraries must be compiled defensively as one would expect, the
biggest issue is thunks! Since thunks passed as arguments can contain
arbitrary code, even code that throws, the compiler have to assume the
worst.

We have not found any sensible way to do static analysis that prove that
a throw at a throw site don't need to reify the stack. We reach the same
conclusion when doing analysis on catch sites. Even if a handler can be
proven to not use stack traces, we might still need reify the stack in
order to pass it down to the next handler.

\subsection{Stack freezing} \label{sec:stack_freezing}

One issue with lazy reification is that the stack is a mutable data
structure, so it is not enough to have a reference to the
topmost chunk. The issue of mutability goes away if we make the stack structure
immutable when reifying\footnote{So far, all stack reifications have been for
  the purpose of creating a stack value. The name reification has been used
  since all methods discussed until now have been about copying the essentials
  of the stack and store it in changed format. To avoid too much terminology in
  this paper, we call \emph{all} methods of creating a stack value for
  ``reification''.}
, we call that \emph{stack freezing}. To freeze the whole stack
would mean to freeze each individual stack chunk. A frozen chunk's
content should be regarded as read-only and the reference to the top
of the stack should not change either. It turns out that thanks to
already existing machinery, stack freezing should be comparably easy to
implement.

Freezing a particular chunk (a \texttt{StgStack} value) would be
trivial by just setting the chunk's \texttt{stack\_size} to be zero
and saving the \texttt{sp}-value to another field. By setting
the \texttt{stack\_size} to zero, the overflow check described in
section~\ref{sec:structure_of_stack} would automatically kick in as a
copy-on-write mechanism. Since we saved the \texttt{sp} value at the time of the reification,
the \texttt{sp} value can change, which means that the stack chunk
can still safely shrink. The stack value itself would be a value with
a reference to the stack chunk that was current at the time of the
exception, as seen in figure~\ref{fig:freezing}.

\begin{figure}
\begin{mdframed}
  \includegraphics[width=5.5in]{build/fig/freezing}
  \caption{Illustration of stack freezing. The frames with \texttt{size = 0}
    are the frames that have been frozen by the run time system when a crash
    was detected.}
  \label{fig:freezing}
\end{mdframed}
\end{figure}

So far this solution is linear, since freezing the whole stack would mean
to traverse through each chunk and freezing them. The number of chunks is
linear in the size of the stack. Luckily, we can get away with freezing
the whole stack by only freezing the chunk where the handler lies.
Because when we get an underflow, control is passed to the RTS
\cite{github_underflow_frame}. From the RTS code we could freeze
the next chunk of the underflow frame, which can be thought of as freezing lazily.

There are multiple drawbacks of freezing the stack. When doing a garbage
collection, a stack value has a reference to the old stack and it would
be wasteful to retain the payloads and all their references if the
eventual printed stack trace is only based on the info pointers. Another
drawback is that it is not clear when (if ever) to \emph{thaw} the
stack. Thawing would be the process of undoing the freezing of the
stack. Freezing the stack and the chunk is very cheap, but a frozen chunk
will cause overflows on every push. Thawing can be done in
constant time by just restoring the \texttt{stack\_size} of the current
stack chunk. Thawing would ideally happen when the last stack value
become unreachable from Haskell code, but it is not obvious how this
would be implemented and if the RTS has support for this.

The overall advantage of stack freezing is that we would never actually
need to create another array, so then all payloads are still accessible.
If the payloads are available from the stack value in the handler,
the optimizations from section~\ref{sec:frames_of_interest} could be
configurable from code in Haskell-land. Even better, one day the stack
traces could utilize the stack frames even more by for example printing
the values of the free variables in a case continuation.

\subsection{Chunked reifying} \label{sec:chunked_reifying}

Chunked reifying of the stack is another idea about doing the stack
reification lazily. In chunked reification we do not freeze the stack, instead, we observe
that the stack is \emph{almost} immutable already! The stack can only
modify itself frame by frame unlike an array which can access any
element. This is a powerful property since this means that the stack is
almost immutable. The idea of chunked reification is that when the stack
pointer moves down towards the bottom of the stack, we start
copying over frames before they get overwritten.

% TODO: Peter commented:
% > > > Not sure I get what you mean. Do you mean on stack underflow?
% > > > How would we know where the reified stuck chunks are at that
% > > > point?
% > >
% > > Yes, I meant for a chunk to be reified on stack underflows. I did not
% > > want to go into detail since I didn't expect such a knowledgeable
% > > reader 
% > > The idea I had was to save them in the StgStack chunks. I
% > > have an idea for how to implement this without locking and using weak
% > > references. Unfortunately that idea only lies in my head... 
% > 
% > So having less knowledge about the subject would make it more clear?
% > No offence, but that makes it sound really shaky.

Luckily, the execution stack as implemented in GHC is already chunked
and we could decide to reify the next chunk on each underflow. Like with
the approach of stack freezing, it would help performance to do something similar
to thawing the stack when stack values become inaccessible. While
chunked reification doesn't freeze the stack it must be able to tell
stack parents that they don't need to reify on underflows when no stack
value is alive anymore.
